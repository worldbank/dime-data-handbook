%------------------------------------------------

\begin{fullwidth}
High quality research begins with a thoughtfully-designed, field-tested survey instrument, and a carefully supervised survey.
Much of the recent push toward credibility in the social sciences has focused on analytical practices.
We contest that credible research depends, first and foremost, on the quality of the raw data. This chapter covers the data generation workflow, from questionnaire design to field monitoring, for electronic data collection.
There are many excellent resources on questionnaire design and field supervision,
but few covering the particularly challenges and opportunities presented by electronic surveys.
As there are many survey software, and the market is rapidly evolving, we focus on workflows and primary concepts, rather than software-specific tools.
The chapter covers questionnaire design, piloting and programming; monitoring data quality during the survey; and how to ensure confidential data is handled securely from collection to storage and sharing.


\end{fullwidth}

%------------------------------------------------
\section{Collecting primary data with development partners}

Primary data is the key to most modern development research.
Often, there is simply no source of reliable official statistics
on the inputs or outcomes we are interested in.
Therefore we undertake to create or obtain new data,
typically in partnership with a local agency or organization.
The intention of primary data collection
is to answer a unique question that cannot be approached in any other way,
so it is important to properly collect and handle that data,
especially when it belongs to or describes people.

\subsection{Data ownership}

Data ownership is a tricky subject,
as many jurisdictions have differing laws regarding data and information,
and you may even be subject to multiple conflicting regulations.
In some places, data is implicitly owned by the people who it is about.
In others, it is owned by the people who collected it.
In still more, it is highly unclear and there may be varying norms.
The best approach is always to consult with a local partner
and to make explicit agreements (including consent, where applicable)
about who owns any data that is collected.
Particularly where personal data, business data, or government data is involved
-- that is, when people are disclosing information to you
that you could not obtain simply by walking around and looking --
you should be extremely clear up front about what will be done with data
so that there is no possibility of confusion down the line.

As with all forms of ownership,
data ownership can include a variety of rights and responsibilities
which can be handled together or separately.
Ownership of data may or may not give you
the right to share that data with other people,
the right to publish or reproduce that data,
or even the right to store data in certain ways or in certain locations.
Again, clarity is key.
If you are not collecting the data directly yourself --
for example, if a government, company, or agency is doing it for you --
make sure that you have an explicit agreement with them
about who owns the resulting data
and what the rights and responsibilities of the data collectors are,
including reuse, storage, and retention or destruction of data.

\subsection{Data licensing agreements}

Data licensing is the formal act of giving some data rights to others
while retaining ownership of a particular dataset.
Whether or not you are the owner of a dataset you want to analyze,
you can enter into a licensing agreement to access it for research purposes.
Similarly, when you own a dataset,
you may be interested in allowing specific people
or the general public to use it for various reasons.
As a researcher, it is your responsibility to respect the rights
of people who own data and people who are described in it;
but it is also your responsibility to make sure
that information is as available and accessible it can be.
These twin responsibilities can and do come into tension,
so it is important to be fully informed about what others are doing
and to fully inform others of what you are doing.
Writing down and agreeing to specific details is a good way of doing that.

When you are licensing someone else's data for research,
keep in mind that they are not likely to be familiar
with the research process, and therefore may be surprised
at some of the things you want to do if you are not clear up front.
You will typically want the right to create and retain
derivative indicators, and you will want to own that output dataset.
You will want to store, catalog, or publish, in whole or in part,
either the original licensed material or the derived dataset.
Make sure that the license you obtain from the data owner allows these uses,
and that you check in with them if you have any questions
about what you are allowed to do with specific portions of their data.

When you are licensing your own data for release,
whether it is to a particular individual or to a group,
make sure you take the same considerations.
Would you be okay with someone else publicly releasing that data in full?
Would you be okay with it being stored on servers anywhere in the world,
even ones that are owned by corporations or governments abroad?
Would you expect that users of your data cite you or give you credit,
or would you require them in turn to release
their derivative data or publications under similar licenses as yours?
Whatever your answers are to these questions,
make sure your license or other agreement
specifically details those requirements.

\subsection{Receiving data from development partners}

Data may be received from development partners in various ways.
You may conduct a first-hand survey either of them or with them
(more on that in the next section).
You may receive access to servers or accounts that already exist.
You may receive a one-time transfer of a block of data,
or you may be given access to a restricted area to extract information.
Talk to an information-technology specialist,
either at your organization or at the partner organization,
to ensure that data is being transferred, received, and stored
in a method that conforms to the relevant level of security.
The data owner will determine the appropriate level of security.
Whether or not you are the data owner, you will need to use your judgment
and follow the data protocols that were determined
in the course of your IRB approval to obtain and use the data:
these may be stricter than the requirements of the data provider.

Another consideration that is important at this stage is proper documentation and cataloging of data.
It is not always clear what pieces of information jointly constitute a ``dataset'',
and many of the sources you receive data from will not be organized for research.
To help you keep organized and to put some structure on the materials you will be receiving,
you should always retain the original data as received
alongside a copy of the corresponding ownership agreement or license.
You should make a simple ``readme'' document noting the date of receipt,
the source and recipient of the data, and a brief description of what it is.
All too often data produced by systems is provided as vaguely-named spreadsheets,
or transferred as electronic communications with non-specific titles,
and it is not possible to keep track of these kinds of information as data over time.
Eventually, you will want to make sure that you are creating a collection or object
that can be properly submitted to a data catalog and given a reference and citation.

As soon as the requisite pieces of information are stored together,
think about which ones are the components of what you would call a dataset.
This is, as many things are, more of an art than a science:
you want to keep things together that belong together,
but you also want to keep things apart that belong apart.
There usually won't be a precise way to tell the answer to this question,
so consult with others about what is the appropriate level of aggregation
for the data project you have endeavored to obtain.
This is the object you will think about cataloging, releasing, and licensing
as you move towards the publication part of the research process.
This may require you to re-check with the provider
about what portions are acceptable to license,
particularly if you are combining various datasets
that may provide even more information about specific individuals.

%------------------------------------------------
\section{Collecting primary data using electronic surveys}

If you are collecting data directly from the research subjects yourself,
you are most likely designing and fielding an electronic survey.
These types of data collection technologies
have greatly accelerated our ability to bring in high-quality data
using purpose-built survey instruments,
and therefore improved the precision of research.
At the same time, electronic surveys create some pitfalls to avoid.
Programming surveys efficiently requires a very different mindset
than simply designing them in word processing software,
and ensuring that they flow correctly and produce data
that can be used in statistical software requires careful organization.
This section will outline the major steps and technical considerations
you will need to follow whenever you field a custom survey instrument.

\subsection{Developing a survey instrument}

A well-designed questionnaire results from careful planning,
consideration of analysis and indicators, close review of existing questionnaires,
survey pilots, and research team and stakeholder review.
There are many excellent resources on questionnaire design,
such as from the World Bank's Living Standards Measurement Survey.\cite{glewwe2000designing}
The focus of this section is the design of electronic field surveys,
often referred to as Computer Assisted Personal Interviews (CAPI).\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Computer-Assisted\_Personal\_Interviews\_(CAPI)}}
Although most surveys are now collected electronically, by tablet, mobile phone or web browser,
\textbf{questionnaire design}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Questionnaire_Design}}
  \index{questionnaire design}
(content development) and questionnaire programming (functionality development)
should be seen as two strictly separate tasks.
Therefore, the research team should agree on all questionnaire content
and design a paper version of the survey before beginning to program the electronic version.
This facilitates a focus on content during the design process
and ensures teams have a readable, printable version of their questionnaire.
Most importantly, it means the research, not the technology, drives the questionnaire design.

We recommend this approach because an easy-to-read paper questionnaire
is especially useful for training data collection staff,
by focusing on the survey content and structure before diving into the technical component.
It is much easier for enumerators to understand the range of possible participant responses
and how to hand them correctly on a paper survey than on a tablet,
and it is much easier for them to translate that logic to digital functionality later.
Finalizing this version of the questionnaire before beginning any programming
also avoids version control concerns that arise from concurrent work
on paper and electronic survey instruments.
Finally, a readable paper questionnaire is a necessary component of data documentation,
since it is difficult to work backwards from the survey program to the intended concepts.

The workflow for designing a questionnaire will feel much like writing an essay, or writing pseudocode:
begin from broad concepts and slowly flesh out the specifics.
It is essential to start with a clear understanding of the
\textbf{theory of change}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Theory_of_Change}}
and \textbf{experimental design} for your project.
The first step of questionnaire design is to list key outcomes of interest,
as well as the main covariates to control for and any variables needed for experimental design.
The ideal starting point for this is a \textbf{pre-analysis plan}.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Pre-Analysis_Plan}}

Use the list of key outcomes to create an outline of questionnaire \textit{modules}.
Do not number the modules yet; instead use a short prefix so they can be easily reordered.
For each module, determine if the module is applicable to the full sample,
the appropriate respondent, and whether or how often, the module should be repeated.
A few examples: a module on maternal health only applies to household with a woman who has children,
a household income module should be answered by the person responsible for household finances,
and a module on agricultural production might be repeated for each crop the household cultivated.
Each module should then be expanded into specific indicators to observe in the field.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Literature_Review_for_Questionnaire}}
At this point, it is useful to do a  \textbf{content-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Piloting_Survey_Content}}
of the questionnaire.
Doing this pilot with a pen-and-paper questionnaire encourages more significant revisions,
as there is no need to factor in costs of re-programming,
and as a result improves the overall quality of the survey instrument.
Questionnaires must also include ways to document the reasons for \textbf{attrition},
treatment \textbf{contamination}, and \textbf{loss to follow-up}.
\index{attrition}\index{contamination}
These are essential data components for completing CONSORT records,
a standardized system for reporting enrollment, intervention allocation, follow-up,
and data analysis through the phases of a randomized trial.\cite{begg1996improving}

Once the content of the survey is drawn up,
the team should conduct a small \textbf{survey pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Survey_Pilot}}
using the paper forms to finalize questionnaire design and detect any content issues.
A content-focused pilot\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Piloting_Survey_Content}}
is best done on pen and paper, before the questionnaire is programmed,
because changes at this point may be deep and structural, which are hard to adjust in code.
The objective is to improve the structure and length of the questionnaire,
refine the phrasing and translation of specific questions,
and confirm coded response options are exhaustive.\sidenote{
  \url{https://dimewiki.worldbank.org/index.php?title=Checklist:_Refine_the_Questionnaire_(Content)&printable=yes}}
In addition, it is an opportunity to test and refine all survey protocols,
such as how units will be sampled or pre-selected units identified.
The pilot must be done out-of-sample,
but in a context as similar as possible to the study sample.
Once the team is satisfied with the content and structure of the survey,
it is time to move on to implementing it electronically.

\subsection{Designing surveys for electronic deployment}

Electronic data collection has great potential to simplify survey implementation and improve data quality.
Electronic questionnaires are typically created in a spreadsheet (e.g. Excel or Google Sheets)
or software-specific form builder, which are accessible even to novice users.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Questionnaire_Programming}}
We will not address software-specific form design in this book;
rather, we focus on coding conventions that are important to follow
for electronic surveys regardless of software choice.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/SurveyCTO_Coding_Practices}}
Survey software tools provide a wide range of features
designed to make implementing even highly complex surveys easy, scalable, and secure.
However, these are not fully automatic: you need to actively design and manage the survey.
Here, we discuss specific practices that you need to follow
to take advantage of electronic survey features
and ensure that the exported data is compatible with your analysis software.

From a data perspective, questions with pre-coded response options
are always preferable to open-ended questions.
The content-based pilot is an excellent time to ask open-ended questions
and refine fixed responses for the final version of the questionnaire --
do not count on coding up lots of free text after a full survey.
Coding responses helps to ensure that the data will be useful for quantitative analysis.
Two examples help illustrate the point.
First, instead of asking ``How do you feel about the proposed policy change?'',
use techniques like \textbf{Likert scales}\sidenote{
  \textbf{Likert scale:} an ordered selection of choices indicating the respondent's level of agreement or disagreement with a proposed statement.}.
Second, if collecting data on medication use or supplies, you could collect:
the brand name of the product; the generic name of the product; the coded compound of the product;
or the broad category to which each product belongs (antibiotic, etc.).
All four may be useful for different reasons,
but the latter two are likely to be the most useful for data analysis.
The coded compound requires providing a translation dictionary to field staff,
but enables automated rapid recoding for analysis with no loss of information.
The generic class requires agreement on the broad categories of interest,
but allows for much more comprehensible top-line statistics and data quality checks.
Rigorous field testing is required to ensure that answer categories are comprehensive;
however, it is best practice to include an \textit{other, specify} option.
Keep track of those responses in the first few weeks of fieldwork.
Adding an answer category for a response frequently showing up as \textit{other} can save time,
as it avoids extensive post-coding.

It is essential to name the fields in your questionnaire
in a way that will also work in your data analysis software.
Most survey programs will not enforce this by default,
since limits vary by software,
and surveys will subtly encourage you to use long sentences
and detailed descriptions of choice options.
This is what you want for the enumerator-respondent interaction,
but you should already have analysis-compatible labels programmed in the background
so the resulting data can be rapidly imported in analytical software.
There is some debate over how exactly individual questions should be identified:
formats like \texttt{hq\_1} are hard to remember and unpleasant to reorder,
but formats like \texttt{hq\_asked\_about\_loans} quickly become cumbersome.
We recommend using descriptive names with clear prefixes so that variables
within a module stay together when sorted alphabetically.\sidenote{
  \url{https://medium.com/@janschenk/variable-names-in-survey-research-a18429d2d4d8}}
Variable names should never include spaces or mixed cases
(we prefer all-lowercase naming).
Take special care with the length: very long names will be cut off in some softwares,
which could result in a loss of uniqueness and lots of manual work to restore compatibility.
We further discourage explicit question numbering, as it discourages re-ordering,
which is a common recommended change after the pilot.
In the case of follow-up surveys, numbering can quickly become convoluted,
too often resulting in uninformative variables names like
\texttt{ag\_15a}, \texttt{ag\_15\_new}, \texttt{ag\_15\_fup2}, and so on.

\subsection{Programming electronic questionnaires}

The starting point for questionnaire programming is therefore a complete paper version of the questionnaire,
piloted for content and translated where needed.
Doing so reduces version control issues that arise from making significant changes
to concurrent paper and electronic survey instruments.
Changing structural components of the survey after programming has been started
often requires the coder to substantially re-work the entire code.
This is because the more efficient way to code surveys is non-linear.
When programming, we do not start with the first question and proceed through to the last question.
Instead, we code from high level to small detail,
following the same questionnaire outline established at design phase.
The outline provides the basis for pseudocode,
allowing you to start with high level structure and work down to the level of individual questions.
This will save time and reduce errors,
particularly where sections or field are interdependent or repeated in complex ways.

Electronic surveys are more than simply a paper questionnaire displayed on a mobile device or web browser.
All common survey software allow you to automate survey logic
and add in hard and soft constraints on survey responses.
These features make enumerators' work easier,
and they create the opportunity to identify and resolve data issues in real-time,
simplifying data cleaning and improving response quality.
Well-programmed questionnaires should include most or all of the following features:

\begin{itemize}
  \item{\textbf{Localizations}}: the survey instrument should display full text questions and responses in the survey language, and it should also have English and code-compatible versions of all text and labels.
	\item{\textbf{Survey logic}}: build in all logic, so that only relevant questions appear, rather than relying on enumerators to follow complex survey logic. This covers simple skip codes, as well as more complex interdependencies (e.g., a child health module is only asked to households that report the presence of a child under 5).
	\item{\textbf{Range checks}}:  add range checks for all numeric variables to catch data entry mistakes (e.g. age must be less than 120).
	\item{\textbf{Confirmation of key variables}}: require double entry of essential information (such as a contact phone number in a survey with planned phone follow-ups), with automatic validation that the two entries match.
	\item{\textbf{Multimedia}}: electronic questionnaires facilitate collection of images, video, and geolocation data directly during the survey, using the camera and GPS built into the tablet or phone.
	\item{\textbf{Preloaded data}}: data from previous rounds or related surveys can be used to prepopulate certain sections of the questionnaire, and validated during the interview.
	\item{\textbf{Filtered response options}}: filters reduce the number of response options dynamically (e.g. filtering the cities list based on the state provided).
	\item{\textbf{Location checks}}: enumerators submit their actual location using in-built GPS, to confirm they are in the right place for the interview.
	\item{\textbf{Consistency checks}}: check that answers to related questions align, and trigger a warning if not so that enumerators can probe further (.e.g., if a household reports producing 800 kg of maize, but selling 900 kg of maize from their own production).
	\item{\textbf{Calculations}}: make the electronic survey instrument do all math, rather than relying on the enumerator or asking them to carry a calculator.
\end{itemize}

All survey softwares include debugging and test options
to correct syntax errors and make sure that the survey instruments will successfully compile.
This is not sufficient, however, to ensure that the resulting dataset
will load without errors in your data analysis software of choice.
We developed the \texttt{ietestform} command,\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/ietestform}}
part of the Stata package \texttt{iefieldkit},
to implement a form-checking routine for \textbf{SurveyCTO},
a proprietary implementation of the \textbf{Open Data Kit (ODK)} software.
Intended for use during questionnaire programming and before fieldwork,
\texttt{ietestform} tests for best practices in coding, naming and labeling, and choice lists.
Although \texttt{ietestform} is software-specific,
many of the tests it runs are general and important to consider regardless of software choice.
To give a few examples, \texttt{ietestform} tests that no variable names exceed
32 characters, the limit in Stata (variable names that exceed that limit will
be truncated, and as a result may no longer be unique).
It checks whether ranges are included for numeric variables.
\texttt{ietestform} also removes all leading and trailing blanks from response lists,
which could be handled inconsistently across software.

A second survey pilot should be done after the questionnaire is programmed.
The objective of this \textbf{data-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/index.php?title=Checklist:_Refine_the_Questionnaire_(Data)&printable=yes}}
is to validate the programming and export a sample dataset.
Significant desk-testing of the instrument is required to debug the programming
as fully as possible before going to the field.
It is important to plan for multiple days of piloting,
so that any further debugging or other revisions to the electronic survey instrument
can be made at the end of each day and tested the following, until no further field errors arise.
The data-focused pilot should be done in advance of enumerator training.

%------------------------------------------------
\section{Data quality assurance and data security}

Whether you are handling data from a partner or collecting it directly,
it is important to make sure that data faithfully reflects ground realities.
Data quality assurance requires a combination of real-time data checks
and back-checks or validation audits, which often means tracking down
the people whose information is in the dataset.
However, since that data also likely contains sensitive or personal information,
it is important to keep it safe throughout the entire process.
All sensitive data must be handled in a way
where there is no risk that anyone who is not approved by an IRB
for the specific project has the ability to access the data.
Data can be sensitive for multiple reasons,
but the most common reasons are that it contains personally identifiable information (PII)
or that the partner providing the data does not want it to be released.
This section will detail principles and practices for the verification and handling of these datasets.

\subsection{Implementing high frequency quality checks}

A key advantage of continuous electronic data intake methods,
as compared to traditional paper surveys and one-time data dumps,
is the ability to access and analyze the data while the project is ongoing.
Data issues can be identified and resolved in real-time.
Designing systematic data checks and running them routinely throughout data intake
simplifies monitoring and improves data quality.
As part of data collection preparation,
the research team should develop a \textbf{data quality assurance plan}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Data_Quality_Assurance_Plan}}.
While data collection is ongoing,
a research assistant or data analyst should work closely with the field team or partner
to ensure that the data collection is progressing correctly,
and set up and perform \textbf{high-frequency checks (HFCs)} with the incoming data.\sidenote{
  \url{https://github.com/PovertyAction/high-frequency-checks/wiki}}

High-frequency checks (HFCs) should carefully inspect key treatment and outcome variables
so that the data quality of core experimental variables is uniformly high,
and that additional effort is centered where it is most important.
Data quality checks should be run on the data every time it is received (ideally on a daily basis)
to flag irregularities in survey progress, sample completeness or response quality.
\texttt{ipacheck}\sidenote{
  \url{https://github.com/PovertyAction/high-frequency-checks}}
is a very useful command that automates some of these tasks,
regardless of the source of the data.

It is important to check continuously that the observations in the data match the intended sample.
Many survey softwares provide some form of case management features
through which sampled units are directly assigned to individual enumerators.
For data recieved from partners this may be harder to validate,
since they are the authoritative source of the data,
so cross-referencing with other data sources may be necessary to validate data.
Even with careful management, it is often the case that raw data includes duplicate or missing entries,
which may occur due to data entry errors or failed submissions to data servers.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Duplicates_and_Survey_Logs}}
\texttt{ieduplicates}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/ieduplicates}}
provides a workflow for collaborating on the resolution of duplicate entries between you and the provider.
Then, observed units in the data must be validated against the expected sample:
this is as straightforward as merging the sample list with the survey data and checking for mismatches.
Reporting errors and duplicate observations in real-time allows the field team to make corrections efficiently.
Tracking data collection progress is important for monitoring attrition,
so that it is clear early on if a change in protocols or additional tracking will be needed.
It is also important to check data collection completion rate
and sample compliance by surveyor and survey team, if applicable,
or compare data missingness across administrative regions,
to identify any clusters that may be providing data of suspect quality.

High frequency checks should also include content-specific data checks.
Electronic survey and data entry software often incorporates many quality control features,
so these checks should focus on issues survey software cannot check automatically.
As most of these checks are project specific,
it is difficult to provide general guidance.
An in-depth knowledge of the questionnaire and a careful examination of the analysis plan
is the best preparation.
Examples include verifying consistency across multiple response fields,
validation of complex calculations like crop yields or medicine stocks (which require unit conversions),
suspicious patterns in survey timing,
or atypical response patters from specific data sources or enumerators.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Monitoring_Data_Quality}}
Electronic data entry software typically provides rich metadata,
which can be useful in assessing data quality.
For example, automatically collected timestamps show when data was submitted
and (for surveys) how long enumerators spent on each question,
and trace histories show how many
times answers were changed before or after the data was submitted.

High-frequency checks will only improve data quality
if the issues they catch are communicated to the data collection team.
There are lots of ways to do this;
what's most important is to find a way to create actionable information for your team.
\texttt{ipacheck}, for example, generates a spreadsheet with flagged errors;
these can be sent directly to the data collection teams.
Many teams choose other formats to display results,
such as online dashboards created by custom scripts.
It is also possible to automate communication of errors to the field team
by adding scripts to link the HFCs with a messaging program such as WhatsApp.
Any of these solutions are possible:
what works best for your team will depend on such variables as
cellular networks in fieldwork areas, whether field supervisors have access to laptops,
internet speed, and coding skills of the team preparing the HFC workflows.

\subsection{Conducting back-checks and data validation}

Careful validation of data is essential for high-quality data.
Since we cannot control natural measurement error
that comes from variation in the realization of key outcomes,
primary data collection provides the opportunity to make sure
that there is no error arising from inaccuracies in the data itself.
\textbf{Back-checks}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Back_Checks}} and
other validation audits help ensure that data collection is following established protocols,
and that data is not fasified, incomplete, or otherwise suspect.
For back-checks and validation audies, a random subset of the main data is selected,
and a subset of information from the full survey is
verified through a brief targeted survey with the original respondent
or a cross-referenced data set from another source.
Design of the back-checks or validations follows the same survey design
principles discussed above: you should use the analysis plan
or list of key outcomes to establish which subset of variables to prioritize,
and similarly focus on errors that would be major flags for poor quality data.

Real-time access to the data massively increases the potential utility of validation,
and both simplifies and improves the rigor of the associated workflows.
You can use the raw primary data to draw the back-check or validation sample;
this ensures that the validation is correctly apportioned across observations.
As soon as checking is complete, the comparator data can be tested against
the original data to identify areas of concern in real-time.
The \texttt{bcstats} command is a useful tool for analyzing back-check data in Stata.\sidenote{
  \url{https://ideas.repec.org/c/boc/bocode/s458173.html}}
Some electronic surveys surveys also provide a unique opportunity
to do audits through audio recordings of the interview,
typically short recordings triggered at random throughout the questionnaire.
\textbf{Audio audits} are a useful means to assess whether enumerators are conducting interviews as expected.
Do note, however, that audio audits must be included in the informed consent for the respondents.

\subsection{Receiving, storing, and sharing data securely}

Primary data collection, whether in surveys or from partners,
almost always includes \textbf{personally-identifiable information (PII)}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Personally_Identifiable_Information_(PII)}}
from the people who are described in the dataset.
PII must be handled with great care at all points in the data collection and management process,
in order to comply with ethical and legal requirements
and to avoid breaches of confidentiality.
Access to PII must be restricted exclusively to team members
who are granted that permission by the applicable Institutional Review Board
or the data licensing agreement with the partner agency.
Research teams must maintain strict protocols for data security at each stage of the process,
including data collection, storage, and sharing.

In field surveys, most common data collection software will automatically \index{encryption}\textbf{encrypt}\sidenote{
\textbf{Encryption:} Methods which ensure that files are unreadable even if laptops
are stolen, databases are hacked, or unauthorized access to the data is obtained in
any other way. \url{https://dimewiki.worldbank.org/wiki/Encryption}}
all data submitted from the field while in transit (i.e., upload or download).\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Encryption\#Encryption\_in\_Transit}}
If this is implemented by the software you are using, then your
data will be encrypted from the time it leaves the device (in tablet-assisted data 
collation) or browser (in web data collection), until it reaches the server. 
Therefore, as long as you are using an established survey software, this step is 
largely taken care of. However, the research team must ensure that all computers, 
tablets, and accounts that are used in data collection have secure a logon 
password and are never left unlocked.

Even though your data is therefore usually safe while it is being transmitted,
it is not automatically secure when it is being stored.
\textbf{Encryption at rest}\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Encryption\#Encryption\_at\_Rest}}
is the only way to ensure that PII data remains private when it is stored on a
server on the internet. You must keep your data encrypted on the data collection server 
whenever PII data is collected. If you do not, the raw data will be accessible by 
individuals who are not approved by your IRB, such as tech support personnel, server 
administrators and other third-party staff. Encryption at rest must be used to make
data files completely unusable without access to a security key specific to that 
data -- a higher level of security than password-protection. Encryption at rest 
requires active participation from the user, and you should be fully aware that 
if your decryption key is lost, there is absolutely no way to recover your data.

You should not assume that your data is encrypted at rest by default because of 
the careful protocols necessary. In most data collection platforms, encryption at 
rest needs to be explicitly enabled and operated by the user. There is no automatic 
way to implement this protocol, because the encryption key that is generated may 
never pass through the hands of a third party, including the data storage application.
Most survey software implement \textbf{asymmetric encryption}\sidenote{\url{
https://dimewiki.worldbank.org/wiki/Encryption\#Asymmetric\_Encryption}} where 
there are two keys in a public/private key pair. Only the private key can be used to
decrypt the encrypted data, and the public key can only be used to encrypt the data.
It is therefore safe to send the public key to the tablet or the browser used to
collect the data. When you enable encryption, the survey software will allow you to
download -- once -- the public/private keyfile pair needed to decrypt the data. You
upload the public key when you start a new survey, and all data collected using that
public key can only be accessed with the private key from that public/private key
pair. You must store the key pair in a secure location, such as a password manager, as
there is no way to access your data if the private key is lost. Make sure you store
keyfiles with descriptive names to match the survey to which they correspond. Any time
anyone accesses the data -- either when viewing it in the browser or downloading it to
your computer -- they will be asked to provide the keyfile. Only project team members
named in the IRB are allowed access to the private keyfile.


For most analytical needs, you will therefore need to create
a copy of the data which has all direct identifiers removed.
This working copy can be stored using unencrypted storage methods,
staff who are not IRB-approved can access and use the data,
and it can be shared with other people involved in the research without strict protocols.
The following workflow allows you to receive data and store it securely,
without compromising data security:

\begin{enumerate}
	\item Download data
	\item Store a ``master'' copy of the data into an encrypted location that will remain accessible on disk and be regularly backed up
	\item Create a ``gold master'' copy of the raw data in a secure location, such as a long-term cloud storage service or an encrypted physical hard drive stored in a separate location. If you remain lucky, you will never have to access this copy -- you just want to know it is there, safe, if you need it.

\end{enumerate}

This handling satisfies the \textbf{3-2-1 rule}: there are
two on-site copies of the data and one off-site copy, so the data can never
be lost in case of hardware failure.\sidenote{
  \url{https://www.backblaze.com/blog/the-3-2-1-backup-strategy/}}
In addition, you should ensure that all teams take basic precautions to ensure the security of data, as most problems are due to human error.
Ideally, the machine hard drives themselves should also be encrypted,
as well as any external hard drives or flash drives used.
All files sent to the field containing PII data, such as sampling lists, must be encrypted.
You must never share passwords by email; rather, use a secure password manager.
This significantly mitigates the risk in case there is a security breach
such as loss, theft, hacking, or a virus, with little impact on day-to-day utilization.

To simplify workflow, it is best to remove PII variables from your data
at the earliest possible opportunity, and save a de-identified copy of the data.
Once the data is de-identified, it no longer needs to be encrypted
-- therefore you can interact with it directly without having to provide the keyfile.
We recommend de-identification in two stages:
an initial process to remove direct identifiers to create a working de-identified dataset,
and a final process to remove all possible identifiers to create a publishable dataset.
The \textbf{initial de-identification} should happen directly after the encrypted data is downloaded to disk.
At this time, for each variable that contains PII, ask: will this variable be needed for analysis?
If not, the variable should be dropped.
Examples include respondent names, enumerator names, interview dates, and respondent phone numbers.
If the variable is needed for analysis, ask:
can I encode or otherwise construct a variable to use for the analysis that masks the PII,
and drop the original variable?
Examples include geocoordinates (after constructing measures of distance or area, drop the specific location), and names for social network analysis (can be encoded to unique numeric IDs).
If PII variables are directly required for the analysis itself,
it will be necessary to keep at least a subset of the data encrypted through the data analysis process.

Flagging all potentially identifying variables in the questionnaire design stage,
as recommended above, simplifies the initial de-identification.
You already have the list of variables to assess,
and ideally have already assessed those against the analysis plan.
If so, all you need to do is write a script to drop the variables that are not required for analysis,
 encode or otherwise mask those that are required, and save a working version of the data.

The \textbf{final de-identification} is a more involved process,
with the objective of creating a dataset for publication
that cannot be manipulated or linked to identify any individual research participant.
You must remove all direct and indirect identifiers, and assess the risk of statistical disclosure.\sidenote{
  \textbf{Disclosure risk:} the likelihood that a released data record can be associated with an individual or organization.}
\index{statistical disclosure}
There will almost always be a trade-off between accuracy and privacy.
For publicly disclosed data, you should favor privacy.
There are a number of useful tools for de-identification: PII scanners for Stata\sidenote{
  \url{https://github.com/J-PAL/stata_PII_scan}}
or R\sidenote{
  \url{https://github.com/J-PAL/PII-Scan}},
and tools for statistical disclosure control.\sidenote{
  \url{https://sdcpractice.readthedocs.io/en/latest/}}
In cases where PII data is required for analysis,
we recommend embargoing the sensitive variables when publishing the data.
Access to the embargoed data could be granted for the purposes of study replication, if approved by an IRB.

When all data collection is complete, the survey team should prepare a final field report,
which should report reasons for any deviations between the original sample and the dataset collected.
Identification and reporting of \textbf{missing data} and \textbf{attrition}
is critical to the interpretation of survey data.
It is important to structure this reporting in a way that not only
groups broad rationales into specific categories
but also collects all the detailed, open-ended responses
to questions the field team can provide for any observations that they were unable to complete.
This reporting should be validated and saved alongside the final raw data, and treated the same way.
This information should be stored as a dataset in its own right
-- a \textbf{tracking dataset} -- that records all events in which survey substitutions
and loss to follow-up occurred in the field and how they were implemented and resolved.
With the raw data securely stored and backed up,
and a de-identified dataset to work with, you are ready to move to data cleaning and analysis.

%------------------------------------------------
