%------------------------------------------------

\begin{fullwidth}
For most research projects, completing a manuscript is not the end of the task.
Academic journals increasingly require submission of a replication package
which contains the code and materials used to create the results.
These represent an intellectual contribution in their own right,
because they enable others to learn from your process
and better understand the results you have obtained.
Holding code and data to the same standards a written work
is a new practice for many researchers.
Publication typically involves multiple iterations of manuscript, 
code, and data files, with inputs from multiple collaborators.
This process can quickly become unwieldy. 
It is in nobody's interest for a skilled and busy researcher
to spend days re-numbering references (and it can take days)
when a small amount of up-front effort can automate the task.

In this chapter, we suggest tools and workflows for efficiently managing collaboration
and ensuring reproducible outputs.
First, we discuss how to use dynamic documents to collaborate on technical writing.
Second, we provide guidelines for preparing a functioning and informative replication package.
If you have organized your analytical work
according to the general principles outlined in earlier chapters,
preparing to release materials will not require
substantial reorganization of the work you have already done.
Hence, this step represents the conclusion of the system
of transparent, reproducible, and credible research we introduced
from the very first chapter of this book.
We include specific guidance on publishing both code and data files,
noting that these can be a significant contribution in addition to analytical results.
In all cases, we note that technology is rapidly evolving
and that the specific tools noted here may not remain cutting-edge,
but the core principles involved in publication and transparency will endure.
\end{fullwidth}

%------------------------------------------------

\section{Collaborating on technical writing}

Development economics research is increasingly a collaborative effort. 
This reflects changes in the economics discipline overall: 
the number of sole-authored papers is decreasing, 
and the majority of recent papers in top journals have three or more 
authors.\sidenote{\url{https://voxeu.org/article/growth-multi-authored-journal-articles-economics}}
As a consequence, manuscripts typically pass back and forth between several writers
before they are ready for publication,
Just as with the preparation of analytical outputs,
effective collaboration requires the adoption of tools and practices 
that enable version control and simultaneous contribution.
\textbf{Dynamic documents} are a way to significantly simplify workflows:
updates to the analytical outputs that appear in these documents, such as tables and figures,
can be passed on to the final output with a single process,
rather than copy-and-pasted or otherwise handled individually.
Managing the writing process in this way
improves organization and reduces error,
such that there is no risk of materials being compiled
with out-of-date results, or of completed work being lost or redundant.

\subsection{Preparing dynamic documents}

Dynamic documents are a broad class of tools that enable a streamlined, reproducible workflow.
The term ``dynamic'' can refer to any document-creation technology
that allows the inclusion of explicitly encoded linkages to raw output files.
This means that, whenever outputs are updated,
the next time the document is loaded or compiled, it will automatically include
all changes made to all outputs without any additional intervention from the user.
This way, updates will never be accidentally excluded,
and updating results will not become more difficult
as the number of inputs grows,
because they are all managed by a single integrated process.

You will note that this is not possible in tools like Microsoft Office,
although there are various tools and add-ons that produce similar functionality,
and we will introduce some later in this book.
In Word, by default, you have to copy and paste each object individually
whenever tables, graphs, or other inputs have to be updated.
This creates complex inefficiency: updates may be accidentally excluded
and ensuring they are not will become more difficult as the document grows.
As time goes on, it therefore becomes more and more likely
that a mistake will be made or something will be missed.
Therefore this is a broadly unsuitable way to prepare technical documents.

The most widely utilized software
for dynamically managing both text and results is \LaTeX\ (pronounced ``lah-tek'').\sidenote{
	\url{https://github.com/worldbank/DIME-LaTeX-Templates}}
\index{\LaTeX}
\LaTeX\ is a document preparation and typesetting system with a unique syntax.
While this tool has a significant learning curve,
its enormous flexibility in terms of operation, collaboration, output formatting, and styling
make it the primary choice for most large technical outputs.
In fact, \LaTeX\ operates behind-the-scenes in many other dynamic document tools (discussed below).
Therefore, we recommend that you learn to use \LaTeX\ directly
as soon as you are able to and provide several resources for doing so in the next section.

There are tools that can generate dynamic documents from within your scripts, 
such as R's RMarkdown\sidenote{\url{https://rmarkdown.rstudio.com}}
and Stata's \texttt{dyndoc}.\sidenote{\url{https://www.stata.com/manuals/rptdyndoc.pdf}}
These tools ``knit'' or ``weave'' text and code together,
and are programmed to insert code outputs in pre-specified locations.
Documents called ``notebooks'' (such as Jupyter\sidenote{\url{https://jupyter.org}}) work similarly,
as they also use the underlying analytical software to create the document.
These tools are usually appropriate for short or informal documents
because it tends to be difficult to edit the content unless using the tool 
and often does not have as extensive formatting option as, for example, Word.

There are also simple tools for dynamic documents
that do not require direct operation of the underlying code or software,
simply access to the updated outputs.
An example of this is Dropbox Paper,
a free online writing tool that allows linkages to files in Dropbox
which are automatically updated anytime the file is replaced.
They have limited functionality in terms of version control and formatting,
and may never include any references to confidential data,
but can be useful for working on informal outputs, such as blogposts,
with collaborators who do not code. 


\subsection{Technical writing with \LaTeX}

\LaTeX\ is billed as a ``document preparation system''.
What this means is worth unpacking.
In {\LaTeX}, instead of writing in a ``what-you-see-is-what-you-get'' mode
as you do in Word or the equivalent,
you write plain text interlaced with coded instructions for formatting
(similar in concept to HTML).
Because it is written in a plain text file format,
\texttt{.tex} can be version-controlled using Git.
This is why it has become the dominant ``document preparation system'' in technical writing.
\LaTeX\ enables automatically-organized documents,
manages tables and figures dynamically,
and includes commands for simple markup
like font styles, paragraph formatting, section headers and the like.
It includes special controls for including tables and figures,
footnotes and endnotes, complex mathematical notation, and automated bibliography preparation.
It also allows publishers to apply global styles and templates to already-written material,
allowing them to reformat entire documents in house styles with only a few keystrokes.

One of the most important tools available in \LaTeX\
is the BibTeX citation and bibliography manager.\sidenote{
  \url{https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.3194&rep=rep1&type=pdf}}
BibTeX keeps all the references you might use in an auxiliary file,
then references them using a simple element typed directly in the document: a \texttt{cite} command.
The same principles that apply to figures and tables are therefore applied here:
You can make changes to the references in one place (the \texttt{.bib} file),
and then everywhere they are used they are updated correctly with one process.
Specifically, \LaTeX\ inserts references in text using the \texttt{\textbackslash cite\{\}} command.
Once this is written, \LaTeX\ automatically pulls all the citations into text
and creates a complete bibliography based on the citations you used whenever you compile the document.
The system allows you to specify exactly how references should be displayed in text
(such as superscripts, inline references, etc.)
as well as how the bibliography should be styled and in what order
(such as Chicago, MLA, Harvard, or other common styles).
This tool is so widely used that it is natively integrated in Google Scholar.
To obtain a reference in the \texttt{.bib} format for any paper you find,
click ``BibTeX'' at the bottom of the Cite window (below the preformatted options).
Then, copy the code directly from Google Scholar into your \texttt{.bib} file.
They will look like the following:

\codeexample{sample.bib}{./code/sample.bib}

\noindent BibTeX citations are then used as follows:

\codeexample{citation.tex}{./code/citation.tex}

With these tools, you can ensure that references are handled
in a format you can manage and control.\cite{flom2005latex}

\LaTeX\ has one more useful trick:
using \textbf{\texttt{pandoc}},\sidenote{
  \url{https://pandoc.org}}
you can translate the raw document into Word
(or a number of other formats)
by running the following code from the command line:

\codeexample{pandoc.sh}{./code/pandoc.sh}

\noindent The last portion after \texttt{csl=} specifies the bibliography style.
You can download a CSL (Citation Styles Library) file\sidenote{
  \url{https://github.com/citation-style-language/styles}}
for nearly any journal and have it applied automatically in this process.
Therefore, even in the case where you are requested to provide
\texttt{.docx} versions of materials to others, or tracked-changes versions,
you can create them effortlessly,
and use external tools like Word's compare feature
to generate integrated tracked versions when needed.

Unfortunately, despite these advantages, \LaTeX\ can be a challenge to set up and use at first,
particularly if you are new to working with plain text code and file management.
It is also unfortunately weak with spelling and grammar checking.
This is because \LaTeX\ requires that all formatting be done in its special code language,
and it is not particularly informative when you do something wrong.
This can be off-putting very quickly for people
who simply want to get to writing, like senior researchers.
While integrated editing and compiling tools like TeXStudio\sidenote{
  \url{https://www.texstudio.org}}
and \texttt{atom-latex}\sidenote{
  \url{https://atom.io/packages/atom-latex}}
offer the most flexibility to work with \LaTeX\ on your computer,
such as advanced integration with Git,
the entire group of writers needs to be comfortable
with \LaTeX\ before adopting one of these tools.
They can require a lot of troubleshooting at a basic level at first,
and non-technical staff may not be willing or able to acquire the required knowledge.
Cloud-based implementations of \LaTex\, discussed in the next section,
allow teams to take advantage of the features of \LaTeX,
without requiring knowledge of the technical details.

\subsection{Getting started with \LaTeX\ in the cloud}

\LaTeX\ is a challenging tool to get started using,
but the control it offers over the writing process is invaluable.
In order to make it as easy as possible for your team
to use \LaTeX\ without all members having to invest in new skills,
we suggest using a cloud-based implementation as your first foray into \LaTeX\ writing.
Most such sites offer a subscription feature with useful extensions and various sharing permissions,
and some offer free-to-use versions with basic tools that are sufficient
for a broad variety of applications,
up to and including writing a complete academic paper with coauthors.

Cloud-based implementations of \LaTeX\ have several advantageous features.
First, since they are completely hosted online,
they avoid the inevitable troubleshooting required to set up a \LaTeX\ installation
on various personal computers run by the different members of a team.
Second, they typically maintain a single, continuously synced, master copy of the document
so that different writers do not create conflicted or out-of-sync copies,
or need to deal with Git themselves to maintain that sync.
Third, they typically allow collaborators to edit in a fashion similar to Google Docs,
though different services vary the number of collaborators and documents allowed at each tier.
Fourth, and most usefully, some implementations provide a ``rich text'' editor
that behaves pretty similarly to familiar tools like Word,
so that collaborators can write text directly into the document without worrying too much
about the underlying \LaTeX\ coding.
Cloud services also usually offer a convenient selection of templates
so it is easy to start up a project and see results right away
without needing to know a lot of the code that controls document formatting.

Cloud-based implementations of \LaTeX\ also have disadvantages.
There is a small amount of up-front learning required,
continous access to the Internet is necessary,
and updating figures and tables requires a bulk file upload that is tough to automate.
A common problem you will face using online editors is special characters
which, because of code functions, need to be handled differently than in Word.
Most critically, the ampersand (\texttt{\&}), percent (\texttt{\%}), and underscore (\texttt{\_})
need to be ``escaped'' (interpreted as text and not code) in order to render.
This is done by by writing a backslash (\texttt{\textbackslash}) before them,
such as writing \texttt{40\textbackslash\%} for the percent sign to appear in text.
Despite this, we believe that with minimal learning and workflow adjustments,
cloud-based implementations are often the easiest way to allow coauthors to write and edit in \LaTeX\,
so long as you make sure you are available to troubleshoot minor issues like these.


%----------------------------------------------------

\section{Preparing a complete replication package}

While we have focused so far on the preparation of written materials for publication,
it is increasingly important for you to consider how you will publish
the data and code you used for your research as well.
More and more major journals are requiring that publications
provide direct links to both the code and data used to create the results,
and some even require being able to reproduce the results themselves
before they will approve a paper for publication.\sidenote{
  \url{https://www.aeaweb.org/journals/policies/data-code}}
If your material has been well-structured throughout the analytical process,
this will only require a small amount of extra work;
if not, paring it down to the ``replication package'' may take some time.
A complete replication package should accomplish several core functions.
It must provide the exact data and code that is used for a paper,
all necessary (de-identified) data for the analysis,
and all code necessary for the analysis.
The code and data should exactly reproduce the raw outputs you have used for the paper,
and the replication file should not include any documentation or data you would not share publicly.
This usually means removing project-related documentation such as contracts
and details of data collection and other field work,
and double-checking all datasets for potentially identifying information.

\subsection{Publishing data for replication}

Publicly documenting all original data generated as part of a research project 
is an important contribution in its own right.
Your paper should clearly cite the data used,
where and how it is stored, and how and under what circumstances it may be accessed.
You may not be able to publish the data itself, 
due to licensing agreements or ethical concerns.
Even if you cannot release data immediately or publicly,
there are often options to catalog or archive the data.
These may take the form of metadata catalogs or embargoed releases.
Such setups allow you to hold an archival version of your data
which your publication can reference,
and provide information about the contents of the datasets
and how future users might request permission to access them
(even if you are not the person to grant that permission).
They can also provide for timed future releases of datasets
once the need for exclusive access has ended.

If your project collected original data,
releasing the cleaned dataset is a significant contribution that can be made
in addition to any publication of analysis results.\sidenote{
  \url{https://www.povertyactionlab.org/sites/default/files/resources/J-PAL-guide-to-publishing-research-data.pdf}}
It allows other researchers to validate the mechanical construction of your results,
to investigate what other results might be obtained from the same population,
and test alternative approaches or answer other questions.
This fosters collaboration and may enable your team to fully explore variables and
questions that you may not have time to focus on otherwise.
There are different options for data publication.
The World Bank's Development Data Hub\sidenote{
	\url{https://datacatalog.worldbank.org}}
includes a Microdata Catalog\sidenote{
\url{https://microdata.worldbank.org}}
and a Geospatial Catalog,
where researchers can publish data and documentation for their projects.\sidenote{
\url{https://dimewiki.worldbank.org/Microdata\_Catalog}
\newline
\url{https://dimewiki.worldbank.org/Checklist:\_Microdata\_Catalog\_submission}
}
The Harvard Dataverse\sidenote{
	\url{https://dataverse.harvard.edu}}
publishes both data and code. 
The Datahub for Field Experiments in Economics and Public Policy\sidenote{\url{https://dataverse.harvard.edu/dataverse/DFEEP}} 
is especially relevant for impact evaluations. 
Both the World Bank Microdata Catalog and the Harvard Dataverse
create data citations for deposited entries.

When your raw data is owned by someone else,
or for any other reason you are not able to publish it,
in many cases you will still have the right to release derivate datasets,
even if it is just the indicators you constructed and their documentation.\sidenote{
  \url{https://guide-for-data-archivists.readthedocs.io}}
If you have questions about your rights over original or derived materials,
check with the legal team at your organization or at the data provider's.
Make sure you have a clear understanding of the rights associated with the data release
and communicate them to any future users of the data.

When you do publish data, you decide how it may be used and what, if any license, you will assign to it.\sidenote{
  \url{https://iatistandard.org/en/guidance/preparing-organisation/organisation-data-publication/how-to-license-your-data}}
Terms of use available in the World Bank Microdata Catalog include, in order of increasing restrictiveness: open access, direct access, and licensed access.\sidenote{
  \url{https://microdata.worldbank.org/index.php/terms-of-use}}
Open Access data is freely available to anyone, and simply requires attribution.
Direct Access data is to registered users who agree to use the data for statistical and scientific research purposes only, 
to cite the data appropriately, and to not attempt to identify respondents or data providers or link to other datasets that could allow for re-identification. 
Licensed access data is restricted to bona fide users, who submit a documented application for how they will use the data and sign an agreement governing data use. 
The user must be acting on behalf of an organization, which will be held responsible in the case of any misconduct.
Keep in mind that you may or may not own your data,
depending on how it was collected,
and the best time to resolve any questions about licensing rights
is at the time that data collection or sharing agreements are signed.

Published data should be released in a widely recognized format.
While software-specific datasets are acceptable accompaniments to the code
(since those precise materials are probably necessary),
you should also consider releasing generic datasets
such as CSV files with accompanying codebooks,
since these can be used by any researcher.
Additionally, you should also release
the data collection instrument or survey questionnaire
so that readers can understand which data components are
collected directly in the field and which are derived.
If possible, you should publish both a clean version of the data
which corresponds exactly to the original database or questionnaire
as well as the constructed or derived dataset used for analysis.
You should also release the code
that constructs any derived measures,
particularly where definitions may vary,
so that others can learn from your work and adapt it as they like.

\subsection{De-identifying data for publication}
Therefore, before publishing data,
you should carefully perform a \textbf{final de-identification}.
Its objective is to create a dataset for publication
that cannot be manipulated or linked to identify any individual research participant.
If you are following the steps outlined in this book,
you have already removed any direct identifiers after collecting the data.
At this stage, however, you should further remove
all indirect identifiers, and assess the risk of statistical disclosure.\sidenote{
	\textbf{Disclosure risk:} the likelihood that a released data record can be associated with an individual or organization.}\index{statistical disclosure}
To the extent required to ensure reasonable privacy,
potentially identifying variables must be further masked or removed.

There are a number of tools developed to help researchers de-identify data
and which you should use as appropriate at that stage of data collection.
These include \texttt{PII\_detection}\sidenote{
	\url{https://github.com/PovertyAction/PII\_detection}}
from IPA,
\texttt{PII-scan}\sidenote{
	\url{https://github.com/J-PAL/PII-Scan}}
from JPAL,
and \texttt{sdcMicro}\sidenote{
	\url{https://sdcpractice.readthedocs.io/en/latest/sdcMicro.html}}
from the World Bank.
\index{anonymization}
The \texttt{sdcMicro} tool, in particular, has a feature
that allows you to assess the uniqueness of your data observations,
and simple measures of the identifiability of records from that.

There will almost always be a trade-off between accuracy and privacy.
For publicly disclosed data, you should favor privacy.
Stripping identifying variables from a dataset may not be sufficient to protect respondent privacy,
due to the risk of re-identification. 
One potential solution is to add noise to data, as the US Census Bureau has proposed.\cite{abowd2018us}
This makes the trade-off between data accuracy and privacy explicit.
But there are not, as of yet, established norms for such ``differential privacy'' approaches:
most approaches fundamentally rely on judging ``how harmful'' information disclosure would be.
The fact remains that there is always a balance between information release (and therefore transparency)
and privacy protection, and that you should engage with it actively and explicitly.
The best thing you can do is make a complete record of the steps that have been taken
so that the process can be reviewed, revised, and updated as necessary.

In cases where PII data is required for analysis,
we recommend embargoing the sensitive variables when publishing the data.
Access to the embargoed data could be granted for specific purposes,
such as a computational reproducibility check required for publication,
if done under careful data security protocols and approved by an IRB.


\subsection{Publishing code for replication}

Before publishing your code, you should edit it for content and clarity
just as if it were written material.
The purpose of releasing code is to allow others to understand
exactly what you have done in order to obtain your results,
as well as to apply similar methods in future projects.
Therefore it should both be functional and readable
(if you've followed the recommendations in this book
this should be easy to do!).
Code is often not written this way when it is first prepared,
so it is important for you to review the content and organization
so that a new reader can figure out what and how your code should do.
Therefore, whereas your data should already be very clean by publication stage,
your code is much less likely to be so. 
This is often where you need to invest time prior to releasing your replication package.

Unlike data, code usually has few legal and privacy constraints to publication.
The research team owns the code in almost all cases,
and code is unlikely to contain identifying information 
(though you must check carefully that it does not).
Publishing code also requires assigning a license to it;
in a majority of cases, code publishers like GitHub
offer extremely permissive licensing options by default.
(If you do not provide a license, nobody can use your code!)

Before releasing the code, make sure it functions identically on a fresh install of your chosen software.
A new user should have no problem getting the code to execute perfectly.
In either a scripts folder or in the root directory,
you should include a master script that allows the reviewer to run the entire project
and re-create all raw outputsby changing only a single line of code: 
the one setting the directory path.
To ensure that your code will run completely on a new computer,
you must install any required user-written commands in the master script
(for example, in Stata using \texttt{ssc install} or \texttt{net install}
and in R include code giving users the option to install packages,
including selecting a specific version of the package if necessary).
In many cases you can even directly provide the underlying code
for any user-installed packages that are needed to ensure forward-compatibility.
Make sure system settings like \texttt{version}, \texttt{matsize}, and \texttt{varabbrev} are set.

Finally, make sure that code inputs and outputs are clearly identified.
A new user should, for example, be able to easily find and remove
any files created by the code so that they can be recreated quickly.
They should also be able to quickly map all the outputs of the code
to the locations where they are placed in the associated published material,
so ensure that the raw components of figures or tables are clearly identified.
Documentation in the master script is often used to indicate this information.
For example, outputs should clearly correspond by name to an exhibit in the paper, and vice versa.
(Supplying a compiling \LaTeX\ document can support this.)
Code and outputs which are not used should be removed before publication.

\subsection{Releasing a replication package}

Once your data and code are polished for public release,
all you need to do is find a place to publish your materials.
This is slightly easier said than done,
as there are a few variables to take into consideration
and, at the time of writing, no global consensus on the best solution.
The technologies available are likely to change dramatically
over the next few years;
the specific solutions we mention here highlight some current approaches
as well as their strengths and weaknesses.
One option is GitHub.
Making a public GitHub repository is completely free.
It can hold any file types,
provide a structured download of your whole project,
and allow others to look at alternate versions or histories easily.
It is straightforward to simply upload a fixed directory to GitHub
apply a sharing license, and obtain a URL for the whole package.
(However, there is a strict size restriction of 100MB per file and
a restriction on the size of the repository as a whole,
so larger projects will need alternative solutions.)
However, GitHub is not ideal for other reasons.
It is not built to hold data in an efficient way
or to manage licenses.
It does not provide a true archive service --
you can change or remove the contents at any time.
It does not assign a permanent digital object identifier (DOI) link for your work.

Another option is the Harvard Dataverse,\sidenote{
  \url{https://dataverse.harvard.edu}}
which is designed to be a citable data repository.
The Open Science Framework\sidenote{
  \url{https://osf.io}}
and ResearchGate.\sidenote{
  \url{https://https://www.researchgate.net}}
can also hold both code and data.

Any of these locations is acceptable --
the main requirement is that the system can handle
the structured directory that you are submitting,
and that it can provide a stable, structured URL for your project
and report exactly what, if any, modifications you have made since initial publication.
You can even combine more than one tool if you prefer,
as long as they clearly point to each other.
For example, one could publish code on GitHub that points to data published on the World Bank MicroData catalog. 

Emerging technologies such as the ``containerization'' approach of Docker or CodeOcean\sidenote{
  \url{https://codeocean.com}}
offer to store both code and data,
and also provide an online workspace in which others can execute and modify your code
without having to download your tools and match your local environment
when packages and other underlying software may have changed since publication.

In addition to code and data,
you may also want to release an author's copy or preprint
of the article itself along with these raw materials.
Check with your publisher before doing so;
not all journals will accept material that has been released.
Therefore you may need to wait until acceptance is confirmed.
This can be done on a number of preprint websites,
many of which are topic-specific.\sidenote{
  \url{https://en.wikipedia.org/wiki/ArXiv}}
You can also use GitHub and link to the PDF file directly
on your personal website or whatever medium you are
sharing the preprint through.
Do not use Dropbox or Google Drive for this purpose:
many organizations do not allow access to these tools,
and staff may be blocked from accessing your material.
