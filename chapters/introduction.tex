\begin{fullwidth}
Welcome to \textit{Data for Development Impact}.
This book is intended to teach all users of development data
how to handle data effectively, efficiently, and ethically.
An empirical revolution has changed the face of research economics rapidly over the last decade.
%had to remove cite {\cite{angrist2017economic}} because of full page width
Today, especially in the development subfield, working with raw data --
whether collected through surveys or acquired from ``big'' data sources like sensors, satellites, or call data records --
is a key skill for researchers and their staff.
At the same time, the scope and scale of empirical research projects is expanding:
more people are working on the same data over longer timeframes.
As the ambition of development researchers grows, so too has the complexity of the data
on which they rely to make policy-relevant research conclusions.
Yet there are few guides to the conventions, standards, and best practices
that are fast becoming a necessity for empirical research.
This book aims to fill that gap.

This book is targeted to everyone who interacts with development data:
graduate students, research assistants, policymakers, and empirical researchers.
It covers data workflows at all stages of the research process, from design to data acquisition and analysis.
This book is not sector-specific; it will not teach you econometrics, or how to design an impact evaluation.
There are many excellent existing resources on those topics.
Instead, this book will teach you how to think about all aspects of your research from a data perspective,
how to structure research projects to maximize data quality,
and how to institute transparent and reproducible workflows.
The central premise of this book is that data work is a ``social process'',
in which many people need to have the same idea about what is to be done, and when and where and by whom,
so that they can collaborate effectively on large, long-term research projects.
It aims to be a highly practical resource: we provide code snippets, links to checklists and other practical tools,
and references to primary resources that allow the reader to immediately put recommended processes into practice.

\end{fullwidth}

%------------------------------------------------

\section{Doing credible research at scale}

The team responsible for this book is known as \textbf{DIME Analytics}.\sidenote{
\url{https://www.worldbank.org/en/research/dime/data-and-analytics}}
The DIME Analytics team is part of the \textbf{Development Impact Evaluation (DIME)} Department\sidenote{
\url{https://www.worldbank.org/en/research/dime}}
within the World Bank's \textbf{Development Economics (DEC) Vice Presidency}.\sidenote{
\url{https://www.worldbank.org/en/about/unit/unit-dec}}

DIME generates high-quality and operationally relevant data and research
to transform development policy, help reduce extreme poverty, and secure shared prosperity.
It develops customized data and evidence ecosystems to produce actionable information
and recommend specific policy pathways to maximize impact.
DIME conducts research in 60 countries with 200 agencies, leveraging a
US\$180 million research budget to shape the design and implementation of
US\$18 billion in development finance.
DIME also provides advisory services to 30 multilateral and bilateral development agencies.
Finally, DIME invests in public goods (such as this book) to improve the quality and reproducibility of development research around the world.

DIME Analytics was created to take advantage of the concentration and scale of research at DIME to develop and test solutions,
to ensure high quality data collection and research across the DIME portfolio,
and to make training and tools publicly available to the larger community of development researchers.
\textit{Data for Development Impact} compiles the ideas, best practices and software tools Analytics
has developed while supporting DIME's global impact evaluation portfolio.

The \textbf{DIME Wiki} is one of our flagship products, a free online collection of our resources and best practices.\sidenote{
\url{https://dimewiki.worldbank.org}}
This book complements the DIME Wiki by providing a structured narrative of the data workflow for a typical research project.
We will not give a lot of highly specific details in this text,
but we will point you to where they can be found.\sidenote{Like this:
\url{https://dimewiki.worldbank.org/Primary_Data_Collection}}
Each chapter focuses on one task, providing a primarily narrative account of:
what you will be doing; where in the workflow this task falls;
when it should be done; and how to implement it according to best practices.

We will use broad terminology throughout this book to refer to research team members:
\textbf{principal investigators (PIs)} who are responsible for
the overall design and stewardship of the study;
\textbf{field coordinators (FCs)} who are responsible for
the implementation of the study on the ground;
and \textbf{research assistants (RAs)} who are responsible for
handling data processing and analytical tasks.


\section{Adopting reproducible workflows}
We will provide free, open-source, and platform-agnostic tools wherever possible,
and point to more detailed instructions when relevant.
Stata is the notable exception here due to its current popularity in development economics.
Most tools have a learning and adaptation process,
meaning you will become most comfortable with each tool
only by using it in real-world work.
Get to know them well early on,
so that you do not spend a lot of time learning through trial and error.

While adopting the workflows and mindsets described in this book requires an up-front cost,
it will save you (and your collaborators) a lot of time and hassle very quickly.
In part this is because you will learn how to implement essential practices directly;
in part because you will find tools for the more advanced practices;
and most importantly because you will acquire the mindset of doing research with a high-quality data focus.
We hope you will find this book helpful for accomplishing all of the above,
and that mastery of data helps you make an impact.


\section{Writing reproducible code in a collaborative environment}
Throughout the book, we refer to the importance of good coding practices.
These are the foundation of reproducible and credible data work,
and a core part of the new data science of development research.
Code today is no longer a means to an end (such as a research paper),
rather it is part of the output itself: a means for communicating how something was done,
in a world where the credibility and transparency of data cleaning and analysis is increasingly important.
As this is fundamental to the remainder of the book's content,
we provide here a brief introduction to \textbf{``good'' code} and \textbf{process standardization}.

``Good'' code has two elements: (1) it is correct, i.e. it doesn't produce any errors, 
and (2) it is useful and comprehensible to someone who hasn't seen it before 
(or even yourself a few weeks, months or years later).
Many researchers have been trained to code correctly.
However, when your code runs on your computer and you get the correct results, 
you are only half-done writing \textit{good} code.
Good code is easy to read and replicate, making it easier to spot mistakes.
Good code reduces sampling, randomization, and cleaning errors.
Good code can easily be reviewed by others before it's published and replicated afterwards.

Process standardization means that there is
little ambiguity about how something ought to be done,
and therefore the tools to do it can be set in advance.
Standard processes for code help other people to ready your code.\sidenote{
\url{https://dimewiki.worldbank.org/Stata_Coding_Practices}}
Code should be well-documented, contain extensive comments, and be readable in the sense that others can:
(1) quickly understand what a portion of code is supposed to be doing;
(2) evaluate whether or not it does that thing correctly; and
(3) modify it efficiently either to test alternative hypotheses
or to adapt into their own work.\sidenote{\url{https://kbroman.org/Tools4RR/assets/lectures/07_clearcode.pdf}}

You should think of code in terms of three major elements:
\textbf{structure}, \textbf{syntax}, and \textbf{style}.
We always tell people to ``code as if a stranger would read it''
(from tomorrow, that stranger could be you!).
The \textbf{structure} is the environment your code lives in:
good structure means that it is easy to find individual pieces of code that correspond to tasks.
Good structure also means that functional blocks are sufficiently independent from each other
that they can be shuffled around, repurposed, and even deleted without damaging other portions.
The \textbf{syntax} is the literal language of your code.
Good syntax means that your code is readable
in terms of how its mechanics implement ideas --
it should not require arcane reverse-engineering
to figure out what a code chunk is trying to do.
\textbf{Style}, finally, is the way that the non-functional elements of your code convey its purpose.
Elements like spacing, indentation, and naming (or lack thereof) can make your code much more
(or much less) accessible to someone who is reading it for the first time and needs to understand it quickly and correctly.

/subsection{Code examples}
For some implementation portions where precise code is particularly important,
we will provide minimal code examples either in the book or on the DIME Wiki.
All code guidance is software-agnostic, but code examples are provided in Stata.
In the book, code examples will be presented like the following:

\codeexample{code.do}{./code/code.do}

We ensure that each code block runs independently, is well-formatted,
and uses built-in functions as much as possible.
We will point to user-written functions when they provide important tools.
In particular, we point to two suites of Stata commands developed by DIME Analytics,
\texttt{ietoolkit}\sidenote{\url{https://dimewiki.worldbank.org/ietoolkit}} and
\texttt{iefieldkit},\sidenote{\url{https://dimewiki.worldbank.org/iefieldkit}}
which standardize our core data collection, management, and analysis workflows.
We will comment the code generously (as you should),
but you should reference Stata help-files by writing \texttt{help [command]}
whenever you do not understand the command that is being used.
We hope that these snippets will provide a foundation for your code style.
Providing some standardization to Stata code style is also a goal of this team;
we provide our guidance on this in the DIME Analytics Stata Style Guide in the Appendix.

\section{Outline of this book}

This book covers each stage of an empirical research project, from design to publication.
We start with ethical principles to guide empirical research,
focusing on research transparency and the right to privacy.
In Chapter 1, we outline a set of practices that help to ensure
research participants are appropriately protected and
research consumers can be confident in the conclusions reached.
Chapter 2 will teach you to structure your data work to be efficient,
collaborative and reproducible.
It discusses the importance of planning data work at the outset of the research project --
long before any data is acquired -- and provides suggestions for collaborative workflows and tools.
In Chapter 3, we turn to research design,
focusing specifically on how to measure treatment effects
and structure data for common experimental and quasi-experimental research methods.
We provide an overview of research designs frequently used for
causal inference, and consider implications for data structure.
Chapter 4 concerns sampling and randomization:
how to implement both simple and complex designs reproducibly,
and how to use power calculations and randomization inference
to critically and quantitatively assess
sampling and randomization to make optimal choices when planning studies.

Chapter 5 covers data acquisition. We start with
the legal and institutional frameworks for data ownership and licensing,
dive in depth on collecting high-quality survey data,
and finally discuss secure data handling during transfer, sharing, and storage.
Chapter 6 teaches reproducible and transparent workflows for data processing and analysis,
and provides guidance on de-identification of personally-identified data,
focusing on how to organize data work so that it is easy to code the desired analysis.
In Chapter 7, we turn to publication. You will learn
how to effectively collaborate on technical writing,
how and why to publish data,
and guidelines for preparing functional and informative replication packages.

We hope that by the end of the book,
you will have learned how to handle data more efficiently, effectively and ethically
at all stages of the research process.

\mainmatter
