%------------------------------------------------

\begin{fullwidth}
Sampling, randomization, and power calculations are the core elements of experimental design.
\textbf{Sampling} and \textbf{randomization} determine
which units are observed and in which states.
Each of these processes introduces uncertainty into the final estimates of effect sizes.
Sampling noise produces some probability of
selection of units to measure that will produce significantly wrong estimates, and
randomization noise produces some probability of
placement of units into treatment arms that does the same.
Power calculation is the method by which these probabilities of error are assessed.
Good experimental design has high \textbf{power} -- a low likelihood that these noise parameters
will affect estimates of treatment effects.

Not all studies are capable of achieving traditionally high power:
the possible sampling or treatment assignments may simply be fundamentally too noisy.
This may be especially true for novel or small-scale studies --
things that have never been tried before may be hard to fund or execute at scale.
What is important is that every study includes reasonable estimates of its power,
so that the evidentiary value of its results can be transparently assessed.
Demonstrating that sampling and randomization were taken seriously into consideration
before going to field lends credibility to any research study.
Using these tools to design the most highly-powered experiments possible
maximizes the likelihood that reported effect sizes are accurate.
\end{fullwidth}

%------------------------------------------------
\section{Reproducibility in sampling, randomization, and power calculation}

Reproducibility in statistical programming is absolutely essential.\cite{orozco2018make}
This is especially true when simulating or analyzing random processes,
and sampling, randomization, and power calculation
are the prime examples of these sorts of tasks.
This section is a short introduction to ensuring that code
which generates randomized outputs is reproducible.
There are three key inputs to assuring reproducibility in these processes:
\textbf{versioning}, \textbf{sorting}, and \textbf{seeding}.
\index{reproducible randomization}
Without these, other people running your code may get very different results in the future.

\textbf{Versioning} means using the same version of the software.
(All software versions of Stata above version 13 currently operate identically on all platforms.)
If anything is different, the underlying randomization algorithms may have changed,
and it will be impossible to recover the original result.
In Stata, the \texttt{version} command ensures that the software algorithm is fixed.
The \texttt{ieboilstart} command in \texttt{ietoolkit} provides functionality to support this requirement.\sidenote{\url{https://dimewiki.worldbank.org/wiki/ieboilstart}}
In general, you will use \texttt{ieboilstart} at the beginning of your master do-file\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Do-files}}
to set the version once; in this guide, we will use
\texttt{version 13.1} in examples where we expect this to already be done.

\textbf{Sorting} means that the actual data that the random process is run on is fixed.
Most random outcomes have as their basis an algorithmic sequence of pseudorandom numbers.
This means that if the start point is set, the full sequence of numbers will not change.
A corollary of this is that the underlying data must be unchanged between runs:
to ensure that the dataset is fixed, you must make a \texttt{LOCKED} copy of it at runtime.
However, if you re-run the process with the dataset in a different order,
the same numbers will get assigned to different units, and the randomization will turn out different.
In Stata, \texttt{isid [id\_variable], sort} will ensure that order is fixed over repeat runs.

\textbf{Seeding} means manually setting the start-point of the underlying randomization algorithm.
You can draw a standard seed randomly by visiting \url{http://bit.ly/stata-random}.
You will see in the code below that we include the timestamp for verification.
Note that there are two distinct concepts referred to here by ``randomization'':
the conceptual process of assigning units to treatment arms,
and the technical process of assigning random numbers in statistical software,
which is a part of all tasks that include a random component.\sidenote{\url{https://blog.stata.com/2016/03/10/how-to-generate-random-numbers-in-stata/}}
If the randomization seed for the statistical software is not set,
then its pseudorandom algorithm will pick up where it left off.
By setting the seed, you force it to restart from the same point.
In Stata, \texttt{set seed [seed]} will accomplish this.

The code below code loads and sets up the \texttt{auto.dta} dataset
for any random process. Note the three components: versioning, sorting, and seeding.
Why are \texttt{check1} and \texttt{check3} the same? Why is \texttt{check2} different?

\codeexample{replicability.do}{./code/replicability.do}

Commands like \texttt{bys:} and \texttt{merge} will re-sort your data as part of their execution,
To reiterate: any process that includes a random component
is a random process, including sampling, randomization, power calculation,
and many algorithms like bootstrapping.
and other commands may alter the seed without you realizing it.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Randomization_in_Stata}}
Any of these things will cause the output to fail to replicate.
Therefore, each random process should be independently executed
to ensure that these three rules are followed.
Before shipping the results of any random process,
save the outputs of the process in a temporary location,
re-run the file, and use \texttt{cf \_all using [dataset]} targeting the saved file.
If there are any differences, the process has not reproduced,
and \texttt{cf} will return an error, as shown here.

\codeexample{randomization-cf.do}{./code/randomization-cf.do}

%------------------------------------------------
\section{Sampling}

\textbf{Sampling} is the process of randomly selecting units of observation
\index{sampling}
from a master list of individuals to be surveyed survey data collection.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Sampling_\%26_Power_Calculations}}
This list may be called a \textbf{sampling universe}, a \textbf{listing frame}, or something similar.
We recommend that this list be organized in a \textbf{master data set}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Data_Set}},
which will be discussed in chapter ???.
The key content of this list is that it indicates
how many individuals are eligible for sampling and surveying.

The code below demonstrates how to take
a uniform-probability random sample
from a population using the \texttt{sample} command.
More advanced sampling techniques,
such as clustering and stratification,
are in practice identical in implementation
to the randomization section that follows --
instead of creating a \texttt{treatment} variable,
create a \texttt{sample} variable.

\codeexample{simple-sample.do}{./code/simple-sample.do}

The fundamental contribution of sampling to the power of a research design is this:
if you randomly sample a set number of observations from a set frame,
there are a large -- but fixed -- number of sample sets which you may draw.\sidenote{\url{https://davegiles.blogspot.com/2019/04/what-is-permutation-test.html}}
From any large group, you can find some possible sample sets
that have higher-than-average values of some measure;
similarly, you can find some sets that have lower-than-average values.
The variation of these values across the range of all possible sample sets is what creates
\textbf{sampling noise}, the uncertainty in statistical estimates caused by sampling.
\index{sampling noise}

Portions of this noise can be reduced through design choices
such as clustering and stratification.
In general, all sampling requires \textbf{inverse probability weights}.
These are conceptually simple in that the weights for each individual must be precisely the inverse of the probability
with which that individual is included in the sample, but may be practically difficult to calculate for complex methods.
When the sampling probability is uniform, all the weights are equal to one.
Sampling can be structured such that subgroups are guaranteed to appear in a sample:
that is, you can pick ``half the level one facilities and half the level two facilities'' instead of
``half of all facilities''. The key here is that, \textit{for each facility},
the probability of being chosen remains the same -- 0.5.
By contrast, a sampling design that chooses unbalanced proportions of subgroups
has changed the probability that a given individual is included in the sample,
and needs to be reweighted in case you want to calculate overall average statistics.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/tools-of-the-trade-when-to-use-those-sample-weights}}

The sampling noise in the process that we choose
determines the size of the confidence intervals
for any estimates generated from that sample.\sidenote{\url{https://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html}}
In general, for any underlying distribution,
the Central Limit Theorem implies that
the distribution of variation across the possible samples is exactly normal.
Therefore, we can use what are called \textbf{asymptotic standard errors}
to express how far away from the true population parameters our estimates are likely to be.
These standard errors can be calculated with only two datapoints:
the sample size and the standard deviation of the value in the chosen sample.
The code below illustrates the fact that sampling noise
has a distribution in the sense that some actual executions of the sample
give estimation results far from the true value,
and others give results close to it.

\codeexample{sample-noise.do}{./code/sample-noise.do}

The output of the code is a distribution of means in sub-populations of the overall data.
This distribution is centered around the true population mean,
but its dispersion depends on the exact structure of the population.
We use an estimate of the population variation taken from the sample
to assess how far away from that true mean any given sample draw is:
essentially, we estimate the properties of the distribution you see now.
With that estimate, we can quantify the uncertainty in estimates due to sampling noise,
calculate precisely how far away from the true mean
our sample-based estimate is likely to be,
and report that as the standard error of our point estimates.
The interpretation of, say, a 95\% \textbf{confidence interval}
\index{confidence interval}
in this context is that, conditional on our sampling strategy,
we would anticipate that 95\% of future samples from the same distribution
would lead to parameter estimates in the indicated range.
This approach says nothing about the truth or falsehood of any hypothesis.

%------------------------------------------------
\section{Randomization}

\textbf{Randomization} is the process of assigning units to some kind of treatment program.
Many of the Stata techniques shown here can also be used for sampling,
by understanding ``being included in the sample'' as a treatment in itself.
Randomization is used to assign treatment programs in development research
because it guarantees that, \textit{on average},
the treatment will not be correlated with anything it did not cause.\cite{duflo2007using}
However, as you have just seen,
any random process induces noise: so does randomization.
You may get unlucky and create important correlations by chance --
in fact, you can almost always identify some treatment assignment that
creates the appearance of statistical relationships that are not really there.
This section will show you how to assess and control this \textbf{randomization noise}.

To randomize units to treatment arms, we create a randomization \textbf{program}, which
\index{programming}
allows us to re-run the randomization method many times
and assess the amount of randomization noise correctly.\sidenote{\url{https://data.princeton.edu/stata/programming}}
Storing the randomization code as a program allows us to access the whole code block
with a single line of code, so we can tinker with the randomization process
separately from its application to the data.
Programming takes a few lines of code that may be new to you,
but getting used to this structure is very useful.
A simple randomization program is shown below.
This code randomizes observations into two groups by combining
\texttt{xtile} and \texttt{recode},
which can be extended to any proportions for any number of arms.

\codeexample{randomization-program-1.do}{./code/randomization-program-1.do}

With this program created and executed,
the next part of the code, shown below,
can set up for reproducibility.
Then it will call the randomization program by name,
which executes the exact randomization process we programmed
to the data currently loaded in memory.
Having pre-programmed the exact randomization does two things:
it lets us write this next code chunk much more simply,
and it allows us to reuse that precise randomization as needed.
Specifically, the user-written \texttt{ritest} command\sidenote{\url{http://hesss.org/ritest.pdf}}
\index{randomization inference}
allows us to execute a given randomization program repeatedly,
visualize how the randomization might have gone differently,
and calculate alternative p-values against null hypotheses.
These \textbf{randomization inference}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Randomization\_Inference}} significance levels may be very different
than those given by asymptotic confidence intervals,
particularly in small samples (up to several hundred clusters).

After generating the ``true'' treatment assignment,
\texttt{ritest} illustrates the distribution of correlations
that randomization can spuriously produce
between \texttt{price} and \texttt{treatment}.

\codeexample{randomization-program-2.do}{./code/randomization-program-2.do}

\subsection{Clustering and stratification}

To control randomization noise, we often use techniques
\index{clustering}\index{stratification}
that reduce the likelihood of a ``bad draw''.\cite{athey2017econometrics}
These techniques can be used in any random process,
including sampling; their implementation is nearly identical in code.
We mean this in a specific way: we want to exclude
randomizations with certain correlations,
or we want to improve the \textbf{balance}
of the average randomization draw.\cite{bruhn2009pursuit}
The most common is \textbf{stratification},\sidenote{\url{https://dimewiki.worldbank.org/wiki/Stratified_Random_Sample}}
which splits the sampling frame into ``similar'' groups -- \textbf{strata} --
and randomizes \textit{within} each of these groups.
It is important to preserve the overall likelihood for each unit to be included,
otherwise statistical corrections become necessary.
For a simple stratified randomization design,
it is necessary to include strata \textbf{fixed effects},
or an indicator variable for each strata, in any final regression.
This accounts for the fact that randomizations were conducted within the strata,
by comparing each unit to the others within its own strata.

However, manually implementing stratified randomization
is prone to error: in particular, it is difficult to precisely account
for the interaction of group sizes and multiple treatment arms,
particularly when a given strata can contain a small number of clusters,
and when there are a large number of treatment arms.\cite{carril2017dealing}
The user-written \texttt{randtreat} command
properly implements stratification,
with navigable options for handling common pitfalls.
We demonstrate the use of this command here.

\codeexample{randtreat-strata.do}{./code/randtreat-strata.do}

Many studies collect data at a different level of observation than the randomization unit.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Unit_of_Observation}}
For example, a policy may only be able to affect an entire village,
but you are interested in household behavior.
This type of structure is called \textbf{clustering},\sidenote{\url{https://dimewiki.worldbank.org/wiki/Multi-stage_(Cluster)_Sampling}}
because the units are assigned to treatment in clusters.
Because the treatments are assigned in clusters, however,
there are in reality fewer randomized groups than the number of units in the data.
Therefore, standard errors for clustered designs must also be clustered,
at the level at which the treatment was assigned.\sidenote{\url{https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle}}

Clustered randomization must typically be implemented manually;
it typically relies on subsetting the data intelligently
to the desired assignment levels.
We demonstrate here.

\codeexample{randtreat-clusters.do}{./code/randtreat-clusters.do}

%------------------------------------------------
\section{Power calculations}

When we have decided on a practical sampling and randomization design,
we next assess its \textbf{power}.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Power_Calculations_in_Stata}}
\index{power}
Statistical power can be described in a few ways,
each of which has different uses.\sidenote{\url{http://www.stat.columbia.edu/~gelman/stuff_for_blog/chap20.pdf}}
The purpose of power calculations is not to
demonstrate that a study is ``strong'',
but rather to identify where the strengths and weaknesses
of your design are located, so that readers
can correctly assess the evidentiary value of
any results (or null results) in the analysis.
This should be done before going to the field,
across the entire range of research questions
your study might try to answer,
so you know the relative tradeoffs you will face
by changing your sampling and randomization schemes
and can select your final strategy appropriately.

The classic definition of power is
``the likelihood that your design detects a significant treatment effect,
given that there is a non-zero true effect in reality''.
Here we will look at two useful practical applications
of that definition and show what quantitative results can be obtained.
We suggest doing all power calculations by simulation;
you are very unlikely to be able to determine analytically
the power of your study unless you have a very simple design.
Stata has some commands that can calculate power for
very simple designs -- \texttt{power} and \texttt{clustersampsi} --
but they will not answer most of the practical questions
that complex experimental designs require.

\subsection{Minimum detectable effect}

To determine the \textbf{minimum detectable effect}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Minimum_Detectable_Effect}}
\index{minimum detectable effect}
-- the smallest true effect that your design can detect --
conduct a simulation for your actual design.
The structure below uses fake data,
but you should use real data whenever it is available,
or you will have to make assumptions about the distribution of outcomes.
If you are willing to make even more assumptions,
you can use one of the built-in functions.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Power_Calculations_in_Stata}}

Here, we use an outer loop to vary the size of the assumed treatment effect,
which is later used to simulate outcomes in a ``true''
data-generating process (DGP).
The data generating process is written similarly to a
regression model, but it is a separate step.
A data generating process is the ``truth''
that the regression model is trying to estimate.
If our regression results are close to the DGP,
then the regression is ``good'' in the sense we care about.
For each of 100 runs indexed by \texttt{i},
we ask the question: If this DGP were true,
would our design have detected it in this draw?
We run our planned regression model including all controls and store the result,
along with an indicator of the effect size we assumed.

When we have done this 100 times for each effect size we are interested in,
we have built a large matrix of regression results.
That can be loaded into data and manipulated directly,
where each observation represents one possible randomization result.
We flag all the runs where the p-value is significant,
then visualize the proportion of significant results
from each assumed treatment effect size.
Knowing the design's sensitivity to a variety of effect sizes
lets us calibrate whether the experiment we propose
is realistic given the constraints of the amount of data we can collect.

\codeexample{minimum-detectable-effect.do}{./code/minimum-detectable-effect.do}


\subsection{Minimum sample size}

Another way to think about the power of a design
is to figure out how many observations you need to include
to test various hypotheses -- the \textbf{minimum sample size}.
This is an important practical consideration
when you are negotiating funding or submitting proposals,
as it may also determine the number of treatment arms
and types of hypotheses you can test.
The basic structure of the simulation is the same.
Here, we use the outer loop to vary the sample size,
and report significance across those groups
instead of across variation in the size of the effect.

\codeexample{minimum-sample-size.do}{./code/minimum-sample-size.do}


Using the concepts of minimum detectable effect
and minimum sample size together can help answer a key question
that typical approaches to power often do not.
Namely, they can help you determine what tradeoffs to make
in the design of your experiment.
Can you support another treatment arm?
Is it better to add another cluster,
or to sample more units per cluster?
Simultaneously, planning out the regression structure
in advance saves a lot of work once the data is in hand,
and helps you think critically about what you are really testing.
It also helps you to untangle design issues before they occur.
Therefore, simulation-based power analysis is often more of a design aid
than an output for reporting requirements.
At the end of the day, you will probably have reduced
the complexity of your experiment significantly.
For reporting purposes, such as grantwriting and registered reports,
simulation ensures you will have understood the key questions well enough
to report standard measures of power once your design is decided.

%------------------------------------------------
